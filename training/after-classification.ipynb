{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c30c914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "411b2fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e52cdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2307 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=10,\n",
    ")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'spdataset/train',\n",
    "        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "        batch_size=32,\n",
    "        class_mode=\"sparse\",\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e48e124c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'benign': 0, 'malignant': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b265a82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['benign', 'malignant']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = list(train_generator.class_indices.keys())\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33f65cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 329 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=10,\n",
    "        horizontal_flip=True)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        'spdataset/val',\n",
    "        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "        batch_size=32,\n",
    "        class_mode=\"sparse\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58cabd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 661 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=10,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        'spdataset/test',\n",
    "        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "        batch_size=32,\n",
    "        class_mode=\"sparse\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a72b6829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.8313726  0.52156866 0.6039216 ]\n",
      "  [0.8313726  0.52156866 0.6039216 ]\n",
      "  [0.8313726  0.52156866 0.6039216 ]\n",
      "  ...\n",
      "  [0.855915   0.46357006 0.60082495]\n",
      "  [0.8431785  0.46278638 0.6000413 ]\n",
      "  [0.82745105 0.44705886 0.58431375]]\n",
      "\n",
      " [[0.8313726  0.5163261  0.6004266 ]\n",
      "  [0.8313726  0.5180015  0.6015435 ]\n",
      "  [0.8313726  0.5196768  0.6026604 ]\n",
      "  ...\n",
      "  [0.85814875 0.46971294 0.60696787]\n",
      "  [0.8381525  0.45776036 0.5950153 ]\n",
      "  [0.82745105 0.44705886 0.58431375]]\n",
      "\n",
      " [[0.82625    0.5029738  0.59607846]\n",
      "  [0.8279253  0.5052076  0.59607846]\n",
      "  [0.8296007  0.50744134 0.59607846]\n",
      "  ...\n",
      "  [0.86038256 0.47585583 0.6131107 ]\n",
      "  [0.8331266  0.45273438 0.5899893 ]\n",
      "  [0.82745105 0.44705886 0.58431375]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.7960785  0.50980395 0.5882353 ]\n",
      "  [0.7954478  0.50602025 0.5882353 ]\n",
      "  [0.788613   0.4797774  0.5817382 ]\n",
      "  ...\n",
      "  [0.7535319  0.43588477 0.51039463]\n",
      "  [0.7540903  0.43644324 0.510953  ]\n",
      "  [0.75464875 0.4370017  0.5115115 ]]\n",
      "\n",
      " [[0.7960785  0.50980395 0.5882353 ]\n",
      "  [0.79488945 0.5026696  0.5882353 ]\n",
      "  [0.78526235 0.4736345  0.5755953 ]\n",
      "  ...\n",
      "  [0.75952315 0.44187605 0.51638585]\n",
      "  [0.7584063  0.44075915 0.515269  ]\n",
      "  [0.75728935 0.43964228 0.5141521 ]]\n",
      "\n",
      " [[0.7960785  0.50980395 0.5882353 ]\n",
      "  [0.79433095 0.49931896 0.5882353 ]\n",
      "  [0.7819117  0.46749163 0.5694524 ]\n",
      "  ...\n",
      "  [0.7607844  0.4431373  0.5176471 ]\n",
      "  [0.7607844  0.4431373  0.5176471 ]\n",
      "  [0.7607844  0.4431373  0.5176471 ]]]\n"
     ]
    }
   ],
   "source": [
    "for image_batch, label_batch in test_generator:\n",
    "    print(image_batch[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c956ec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shape = (IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "# n_classes = 3\n",
    "\n",
    "# model = models.Sequential([\n",
    "#     layers.InputLayer(input_shape=input_shape),\n",
    "#     layers.Conv2D(32, kernel_size = (3,3), activation='relu'),\n",
    "#     layers.MaxPooling2D((2, 2)),\n",
    "#     layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "#     layers.MaxPooling2D((2, 2)),\n",
    "#     layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "#     layers.MaxPooling2D((2, 2)),\n",
    "#     layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "#     layers.MaxPooling2D((2, 2)),\n",
    "#     layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "#     layers.MaxPooling2D((2, 2)),\n",
    "#     layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "#     layers.MaxPooling2D((2, 2)),\n",
    "#     layers.Flatten(),\n",
    "#     layers.Dense(64, activation='relu'),\n",
    "#     layers.Dense(n_classes, activation='softmax'),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c110f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define constants\n",
    "IMAGE_SIZE = 256  # Adjust this to match your dataset's image size\n",
    "NUM_CLASSES = 2  # Number of classes in your dataset\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "\n",
    "# Load the pre-trained ResNet-50 model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "\n",
    "# Freeze the layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom classification layers on top of the base model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)  # You can adjust the number of units as needed\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "# Create the fine-tuned model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "# Compile the model with 'sparse_categorical_crossentropy' as the loss function\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Save the model\n",
    "# model.save('fine_tuned_resnet50.h5')\n",
    "\n",
    "# Evaluate the model on a test dataset if available\n",
    "# test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "# test_generator = test_datagen.flow_from_directory(\n",
    "#     'test_data_directory',\n",
    "#     target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     class_mode='categorical'\n",
    "# )\n",
    "# test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "# print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ea80186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "72/72 [==============================] - 369s 5s/step - loss: 0.9315 - accuracy: 0.5371 - val_loss: 0.7106 - val_accuracy: 0.5500\n",
      "Epoch 2/30\n",
      "72/72 [==============================] - 357s 5s/step - loss: 0.6999 - accuracy: 0.5468 - val_loss: 0.7867 - val_accuracy: 0.4531\n",
      "Epoch 3/30\n",
      "72/72 [==============================] - 357s 5s/step - loss: 0.6776 - accuracy: 0.5877 - val_loss: 0.6668 - val_accuracy: 0.5594\n",
      "Epoch 4/30\n",
      "72/72 [==============================] - 360s 5s/step - loss: 0.6970 - accuracy: 0.5495 - val_loss: 0.7775 - val_accuracy: 0.4531\n",
      "Epoch 5/30\n",
      "72/72 [==============================] - 349s 5s/step - loss: 0.6763 - accuracy: 0.5934 - val_loss: 0.7919 - val_accuracy: 0.4531\n",
      "Epoch 6/30\n",
      "72/72 [==============================] - 352s 5s/step - loss: 0.6847 - accuracy: 0.5798 - val_loss: 0.6249 - val_accuracy: 0.7688\n",
      "Epoch 7/30\n",
      "72/72 [==============================] - 389s 5s/step - loss: 0.6345 - accuracy: 0.6426 - val_loss: 0.5965 - val_accuracy: 0.7281\n",
      "Epoch 8/30\n",
      "72/72 [==============================] - 362s 5s/step - loss: 0.6162 - accuracy: 0.6848 - val_loss: 0.5939 - val_accuracy: 0.6812\n",
      "Epoch 9/30\n",
      "72/72 [==============================] - 358s 5s/step - loss: 0.6079 - accuracy: 0.6681 - val_loss: 0.5768 - val_accuracy: 0.7656\n",
      "Epoch 10/30\n",
      "72/72 [==============================] - 356s 5s/step - loss: 0.6362 - accuracy: 0.6321 - val_loss: 0.5871 - val_accuracy: 0.7500\n",
      "Epoch 11/30\n",
      "72/72 [==============================] - 355s 5s/step - loss: 0.6079 - accuracy: 0.6629 - val_loss: 0.5638 - val_accuracy: 0.7375\n",
      "Epoch 12/30\n",
      "72/72 [==============================] - 352s 5s/step - loss: 0.5962 - accuracy: 0.6875 - val_loss: 0.5597 - val_accuracy: 0.7281\n",
      "Epoch 13/30\n",
      "72/72 [==============================] - 355s 5s/step - loss: 0.6027 - accuracy: 0.6747 - val_loss: 0.5832 - val_accuracy: 0.6656\n",
      "Epoch 14/30\n",
      "72/72 [==============================] - 351s 5s/step - loss: 0.5938 - accuracy: 0.6888 - val_loss: 0.5397 - val_accuracy: 0.7312\n",
      "Epoch 15/30\n",
      "72/72 [==============================] - 350s 5s/step - loss: 0.5797 - accuracy: 0.7042 - val_loss: 0.6278 - val_accuracy: 0.6281\n",
      "Epoch 16/30\n",
      "72/72 [==============================] - 352s 5s/step - loss: 0.5746 - accuracy: 0.6963 - val_loss: 0.5383 - val_accuracy: 0.7156\n",
      "Epoch 17/30\n",
      "72/72 [==============================] - 354s 5s/step - loss: 0.5532 - accuracy: 0.7253 - val_loss: 0.5216 - val_accuracy: 0.7375\n",
      "Epoch 18/30\n",
      "72/72 [==============================] - 358s 5s/step - loss: 0.6094 - accuracy: 0.6756 - val_loss: 0.5572 - val_accuracy: 0.7281\n",
      "Epoch 19/30\n",
      "72/72 [==============================] - 360s 5s/step - loss: 0.5845 - accuracy: 0.6919 - val_loss: 0.5411 - val_accuracy: 0.7625\n",
      "Epoch 20/30\n",
      "72/72 [==============================] - 482s 7s/step - loss: 0.5848 - accuracy: 0.6862 - val_loss: 0.5307 - val_accuracy: 0.7469\n",
      "Epoch 21/30\n",
      "72/72 [==============================] - 566s 8s/step - loss: 0.5889 - accuracy: 0.6923 - val_loss: 0.5332 - val_accuracy: 0.7188\n",
      "Epoch 22/30\n",
      "72/72 [==============================] - 353s 5s/step - loss: 0.6196 - accuracy: 0.6492 - val_loss: 0.5451 - val_accuracy: 0.7469\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps= validation_generator.samples // BATCH_SIZE,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14933958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.applications import ResNet50\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# # Define constants\n",
    "# IMAGE_SIZE = 256  # Adjust this to match your dataset's image size\n",
    "# NUM_CLASSES = 2  # Number of classes in your dataset\n",
    "# BATCH_SIZE = 32\n",
    "# EPOCHS = 20\n",
    "\n",
    "# # Load the pre-trained ResNet-50 model\n",
    "# base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "\n",
    "# # Freeze the layers of the base model\n",
    "# for layer in base_model.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# # Add custom classification layers on top of the base model\n",
    "# x = base_model.output\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# x = Dense(1024, activation='relu')(x)  # You can adjust the number of units as needed\n",
    "# predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "# # Create the fine-tuned model\n",
    "# model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# # Compile the model\n",
    "# # Compile the model with 'sparse_categorical_crossentropy' as the loss function\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Create data generators for training and validation\n",
    "# train_datagen = ImageDataGenerator(\n",
    "#     rescale=1.0 / 255,\n",
    "#     rotation_range=20,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     shear_range=0.2,\n",
    "#     zoom_range=0.2,\n",
    "#     horizontal_flip=True,\n",
    "#     fill_mode='nearest'\n",
    "# )\n",
    "\n",
    "# # Assuming you have a directory structure with train and validation subdirectories\n",
    "# train_generator = train_datagen.flow_from_directory(\n",
    "#     'spdataset/train',\n",
    "#     target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     class_mode='sparse'  # Use 'sparse' for a small number of classes\n",
    "# )\n",
    "\n",
    "# # Validation data generator\n",
    "# validation_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "# validation_generator = validation_datagen.flow_from_directory(\n",
    "#     'spdataset/val',\n",
    "#     target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     class_mode='sparse'  # Use 'sparse' for a small number of classes\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(\n",
    "#     train_generator,\n",
    "#     steps_per_epoch=len(train_generator),\n",
    "#     epochs=10,\n",
    "#     validation_data=validation_generator,\n",
    "#     validation_steps=len(validation_generator)\n",
    "# )\n",
    "\n",
    "# # # Save the model\n",
    "# # model.save('fine_tuned_resnet50.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ff939f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 254, 254, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 127, 127, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 62, 62, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 60, 60, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 30, 30, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 14, 14, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 6, 6, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 2, 2, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183747 (717.76 KB)\n",
      "Trainable params: 183747 (717.76 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be44b098",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ce00bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "72/72 [==============================] - 204s 3s/step - loss: 0.6931 - accuracy: 0.5499 - val_loss: 0.5323 - val_accuracy: 0.7312\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 180s 2s/step - loss: 0.5138 - accuracy: 0.7556 - val_loss: 0.4805 - val_accuracy: 0.7937\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 175s 2s/step - loss: 0.4894 - accuracy: 0.7657 - val_loss: 0.4375 - val_accuracy: 0.7969\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 181s 3s/step - loss: 0.4516 - accuracy: 0.7785 - val_loss: 0.4016 - val_accuracy: 0.8281\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 186s 3s/step - loss: 0.4368 - accuracy: 0.7833 - val_loss: 0.3788 - val_accuracy: 0.8438\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 177s 2s/step - loss: 0.4331 - accuracy: 0.7815 - val_loss: 0.4069 - val_accuracy: 0.8125\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 176s 2s/step - loss: 0.4240 - accuracy: 0.7956 - val_loss: 0.3897 - val_accuracy: 0.8344\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 172s 2s/step - loss: 0.3992 - accuracy: 0.8044 - val_loss: 0.4357 - val_accuracy: 0.8125\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 175s 2s/step - loss: 0.4357 - accuracy: 0.7833 - val_loss: 0.4090 - val_accuracy: 0.8031\n",
      "Epoch 10/10\n",
      "72/72 [==============================] - 173s 2s/step - loss: 0.4052 - accuracy: 0.8088 - val_loss: 0.3782 - val_accuracy: 0.8250\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // 32,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps= validation_generator.samples // 32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3945423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/3\\assets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "model_version=max([int(i) for i in os.listdir(\"../models\") + [0]])+1\n",
    "model.save(f\"../models/{model_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9dcfbf2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../potatoesv2.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save(\"../potatoesv2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21267ef8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images_batch, labels_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtest_ds\u001b[49m\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      4\u001b[0m     first_image \u001b[38;5;241m=\u001b[39m images_batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m     first_label \u001b[38;5;241m=\u001b[39m labels_batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_ds' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b194794a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
